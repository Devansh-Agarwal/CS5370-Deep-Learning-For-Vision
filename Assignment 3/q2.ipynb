{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "import math  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradDescent(lamb1, lamb2, lr):\n",
    "    w1 = 3\n",
    "    w2 = 4\n",
    "    error = .000001\n",
    "    change = 10000000\n",
    "    iteration = 1\n",
    "    while change > error and iteration < 10000:\n",
    "        w1old = w1\n",
    "        w2old = w2\n",
    "        w1 = w1 - lr * (lamb1 * w1)\n",
    "        w2 = w2 - lr * (lamb2 * w2)\n",
    "        if(w1 > 10e5 or w2 > 10e5):\n",
    "            print(\"Divergence at iteratoin = \", iteration, \" learning rate = \", lr)\n",
    "            break\n",
    "        change = math.sqrt((w1 - w1old)**2 + (w2 - w2old)**2)\n",
    "#         print(\"learnging rate = \", lr, \" iteration:\", iteration, \"w1 =\", w1, \"w2 = \", w2)\n",
    "        iteration += 1\n",
    "    print(\"learnging rate = \", lr, \" iteration:\", iteration, \"w1 =\", w1, \"w2 = \", w2, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running multiple times with different lamda values and increasing the ratio lamb2/lamb1 we verify that the largest value of the learning rate is governed by this ratio as the ratio increases the largest learning rate decreases. It is varified that the learning rate should be less than lamb1/lamb2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learnging rate =  0.0001  iteration: 8005 w1 = 0.0009983642369687268 w2 =  4.63615076057443e-35\n",
      "\n",
      "\n",
      "\n",
      "learnging rate =  0.001  iteration: 1028 w1 = 9.873385779837467e-05 w2 =  4.065530144113866e-47\n",
      "\n",
      "\n",
      "\n",
      "learnging rate =  0.01  iteration: 122 w1 = 8.718964248596092e-06 w2 =  0.0\n",
      "\n",
      "\n",
      "\n",
      "Divergence at iteratoin =  6  learning rate =  0.1\n",
      "learnging rate =  0.1  iteration: 6 w1 = 0.0 w2 =  2125764.0\n",
      "\n",
      "\n",
      "\n",
      "Divergence at iteratoin =  2  learning rate =  10\n",
      "learnging rate =  10  iteration: 2 w1 = 29403 w2 =  3992004\n",
      "\n",
      "\n",
      "\n",
      "Divergence at iteratoin =  2  learning rate =  20\n",
      "learnging rate =  20  iteration: 2 w1 = 118803 w2 =  15984004\n",
      "\n",
      "\n",
      "\n",
      "Divergence at iteratoin =  2  learning rate =  100\n",
      "learnging rate =  100  iteration: 2 w1 = 2994003 w2 =  399920004\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.0001, 0.001, 0.01, 0.1, 10, 20, 100]:\n",
    "    gradDescent(10, 100, lr)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learnging rate =  0.0001  iteration: 8005 w1 = 0.0009983642369687268 w2 =  2e-323\n",
      "\n",
      "\n",
      "\n",
      "learnging rate =  0.001  iteration: 1028 w1 = 9.873385779837467e-05 w2 =  0.0\n",
      "\n",
      "\n",
      "\n",
      "Divergence at iteratoin =  6  learning rate =  0.01\n",
      "learnging rate =  0.01  iteration: 6 w1 = 1.5943230000000002 w2 =  2125764.0\n",
      "\n",
      "\n",
      "\n",
      "Divergence at iteratoin =  4  learning rate =  0.1\n",
      "learnging rate =  0.1  iteration: 4 w1 = 0.0 w2 =  384238404.0\n",
      "\n",
      "\n",
      "\n",
      "Divergence at iteratoin =  2  learning rate =  10\n",
      "learnging rate =  10  iteration: 2 w1 = 29403 w2 =  399920004\n",
      "\n",
      "\n",
      "\n",
      "Divergence at iteratoin =  2  learning rate =  20\n",
      "learnging rate =  20  iteration: 2 w1 = 118803 w2 =  1599840004\n",
      "\n",
      "\n",
      "\n",
      "Divergence at iteratoin =  2  learning rate =  100\n",
      "learnging rate =  100  iteration: 2 w1 = 2994003 w2 =  39999200004\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.0001, 0.001, 0.01, 0.1, 10, 20, 100]:\n",
    "    gradDescent(10, 1000, lr)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learnging rate =  0.0001  iteration: 8005 w1 = 0.0009983642369687268 w2 =  0.0\n",
      "\n",
      "\n",
      "\n",
      "Divergence at iteratoin =  6  learning rate =  0.001\n",
      "learnging rate =  0.001  iteration: 6 w1 = 2.8244404482030006 w2 =  2125764.0\n",
      "\n",
      "\n",
      "\n",
      "Divergence at iteratoin =  4  learning rate =  0.01\n",
      "learnging rate =  0.01  iteration: 4 w1 = 1.9683000000000002 w2 =  384238404.0\n",
      "\n",
      "\n",
      "\n",
      "Divergence at iteratoin =  2  learning rate =  0.1\n",
      "learnging rate =  0.1  iteration: 2 w1 = 0.0 w2 =  3992004.0\n",
      "\n",
      "\n",
      "\n",
      "Divergence at iteratoin =  2  learning rate =  10\n",
      "learnging rate =  10  iteration: 2 w1 = 29403 w2 =  39999200004\n",
      "\n",
      "\n",
      "\n",
      "Divergence at iteratoin =  2  learning rate =  20\n",
      "learnging rate =  20  iteration: 2 w1 = 118803 w2 =  159998400004\n",
      "\n",
      "\n",
      "\n",
      "Divergence at iteratoin =  2  learning rate =  100\n",
      "learnging rate =  100  iteration: 2 w1 = 2994003 w2 =  3999992000004\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.0001, 0.001, 0.01, 0.1, 10, 20, 100]:\n",
    "    gradDescent(10, 10000, lr)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchEnv",
   "language": "python",
   "name": "pytorchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
